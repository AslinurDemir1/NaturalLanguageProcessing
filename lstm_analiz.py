import torch
import torch.nn as nn
import torch.optim as optim

# --- 1. VERÄ° HAZIRLIÄI ---
# Veri setimiz (CÃ¼mle, Etiket)
# 1: Olumlu, 0: Olumsuz
train_data = [
    ("bu film gerÃ§ekten harika ve sÃ¼rÃ¼kleyici", 1),
    ("oyunculuklar berbat senaryo Ã§ok kÃ¶tÃ¼", 0),
    ("hayatÄ±mda izlediÄŸim en iyi film", 1),
    ("zaman kaybÄ± sakÄ±n izlemeyin", 0),
    ("kurgu muazzam efektler Ã§ok baÅŸarÄ±lÄ±", 1),
    ("hiÃ§ beÄŸenmedim Ã§ok sÄ±kÄ±cÄ±ydÄ±", 0),
    ("sonu Ã§ok saÃ§ma bitti", 0),
    ("mutlaka izlenmesi gereken bir baÅŸyapÄ±t", 1)
]

# SÃ¶zlÃ¼k OluÅŸturma (Kelimeleri SayÄ±ya Ã‡evirme)
word_to_ix = {"<PAD>": 0} # Dolgu (Padding) iÃ§in Ã¶zel karakter
for sent, label in train_data:
    for word in sent.split():
        if word not in word_to_ix:
            word_to_ix[word] = len(word_to_ix)

print(f"SÃ¶zlÃ¼k Boyutu: {len(word_to_ix)}")

# --- 2. MÃœHENDÄ°SLÄ°K KISMI: PADDING (DOLGU) ---
# LSTM, girdilerin hepsinin aynÄ± boyda olmasÄ±nÄ± ister.
# KÄ±sa cÃ¼mlelerin sonuna 0 (PAD) ekleyerek hepsini en uzun cÃ¼mleye eÅŸitleyeceÄŸiz.
MAX_LEN = 6  # CÃ¼mleleri 6 kelimeye sabitleyelim

def prepare_sequence(seq, to_ix):
    idxs = []
    for w in seq.split():
        if w in to_ix:
            idxs.append(to_ix[w])
        else:
            idxs.append(0) # Bilinmeyen kelime (veya PAD)
            
    # EÄŸer 6'dan kÄ±saysa sonuna 0 ekle
    if len(idxs) < MAX_LEN:
        idxs += [0] * (MAX_LEN - len(idxs))
    # EÄŸer 6'dan uzunsa kes
    else:
        idxs = idxs[:MAX_LEN]
        
    return torch.tensor(idxs, dtype=torch.long).view(1, -1) # [1, 6] boyutunda

# --- 3. LSTM MODEL MÄ°MARÄ°SÄ° ---
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMClassifier, self).__init__()
        
        # 1. Embedding KatmanÄ±: Kelime ID -> VektÃ¶r
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # 2. LSTM KatmanÄ±: VektÃ¶r Dizisi -> HafÄ±za Ã–zeti
        # batch_first=True: Girdi [Batch, Uzunluk, Ã–zellik] formatÄ±nda olsun
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        
        # 3. Ã‡Ä±ktÄ± KatmanÄ±: HafÄ±za -> Karar (0/1)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: [1, 6] (Kelimelerin ID'leri)
        
        embeds = self.embedding(x) 
        # embeds: [1, 6, 10] (Her kelime 10'luk vektÃ¶r oldu)
        
        # LSTM Ã‡alÄ±ÅŸÄ±yor...
        # out: TÃ¼m adÄ±mlarÄ±n Ã§Ä±ktÄ±sÄ±
        # hidden: KÄ±sa vadeli hafÄ±za (Son durum)
        # cell: Uzun vadeli hafÄ±za (HÃ¼cre durumu)
        lstm_out, (hidden, cell) = self.lstm(embeds)
        
        # Bize sadece son adÄ±mÄ±n hafÄ±zasÄ± lazÄ±m (CÃ¼mlenin Ã¶zeti)
        # hidden[0]: [1, 1, 16] -> [1, 16]
        final_hidden = hidden[-1] 
        
        # Karar ver
        prediction = self.fc(final_hidden)
        return self.sigmoid(prediction)

# --- 4. MODEL AYARLARI ---
EMBEDDING_DIM = 10  # Her kelime 10 sayÄ±lÄ±k bir vektÃ¶r olsun
HIDDEN_DIM = 16     # LSTM'in hafÄ±zasÄ±nda 16 Ã¶zellik tutulsun
OUTPUT_DIM = 1      # Ã‡Ä±ktÄ± tek bir sayÄ± (0-1 arasÄ± olasÄ±lÄ±k)

model = LSTMClassifier(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)
loss_function = nn.BCELoss() # Binary Cross Entropy (Ä°kili SÄ±nÄ±flandÄ±rma)
optimizer = optim.Adam(model.parameters(), lr=0.01)

# --- 5. EÄÄ°TÄ°M ---
print("\nLSTM EÄŸitiliyor...")
for epoch in range(300): # 300 tur
    total_loss = 0
    for sentence, label in train_data:
        model.zero_grad()
        
        # Veriyi hazÄ±rla
        inputs = prepare_sequence(sentence, word_to_ix)
        target = torch.tensor([[float(label)]]) # Hedef: [1.0] veya [0.0]
        
        # Ä°leri ve Geri
        y_pred = model(inputs)
        loss = loss_function(y_pred, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
    if (epoch+1) % 50 == 0:
        print(f"Tur {epoch+1}, Hata: {total_loss:.4f}")

# --- 6. TEST ZAMANI ---
print("\n--- GerÃ§ek Test ---")

def test_et(cumle):
    model.eval() # DeÄŸerlendirme modu
    with torch.no_grad():
        inputs = prepare_sequence(cumle, word_to_ix)
        score = model(inputs).item()
        
        durum = "OLUMLU ğŸ˜„" if score > 0.5 else "OLUMSUZ ğŸ˜¡"
        print(f"CÃ¼mle: '{cumle}'")
        print(f"Skor: %{score*100:.2f} -> {durum}")
        print("-" * 30)

test_et("bu film harika")
test_et("senaryo Ã§ok sÄ±kÄ±cÄ±ydÄ±")
test_et("efektler baÅŸarÄ±lÄ±")
test_et("zaman kaybÄ±")
# GÃ¶rmediÄŸi bir cÃ¼mle deneyelim
test_et("oyunculuklar Ã§ok kÃ¶tÃ¼")